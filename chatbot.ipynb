{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxqNz9vTZO+ISFtFa5E5ux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hesham81/AI/blob/developers/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# PreProcessing Stage\n"
      ],
      "metadata": {
        "id": "BrYUmOJ5QtaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working With Testing Data"
      ],
      "metadata": {
        "id": "cBkEqjuc9tts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        " return stemmer.stem(word.lower())\n",
        "\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "  tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "  # pass\n",
        "\n",
        "\n",
        "a = \"How Long Does  shipping take ? \"\n",
        "print(a)\n",
        "a = tokenize(a)\n",
        "print(a)\n"
      ],
      "metadata": {
        "id": "0aihYjT8QzqQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "6d149c61-7ecf-4dd4-b6d4-540f402e972f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How Long Does  shipping take ? \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-29a355a76edf>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"How Long Does  shipping take ? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-29a355a76edf>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "# Download the 'punkt_tab' resource if not already present\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        " return stemmer.stem(word.lower())\n",
        "\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "  tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "  bag = np.zeros(len(all_words), dtype=np.float32)\n",
        "  for idx, w in enumerate(all_words):\n",
        "    if w in tokenized_sentence:\n",
        "      bag[idx] = 1.0\n",
        "  return bag\n",
        "\n",
        "\n",
        "\n",
        "sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
        "words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
        "bag = bag_of_words(sentence, words)\n",
        "print(bag)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vYqZJ5MM92yl",
        "outputId": "09558a25-7eca-48a0-c74b-d8c4d1f8268f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 1. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working With The Data Of Json\n",
        "and also working with train\n",
        "**Contain Creating The DataSet**"
      ],
      "metadata": {
        "id": "34Wmmqb0BnsU"
      }
    },
    {
      "source": [
        "import json\n",
        "from nltk_utils import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "with open('intents_training.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    # The problem was here. You need to iterate through intent['patterns'] not intents['patterns']\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "ignore_words = ['? ', '!', ',', '.']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n",
        "print(all_words)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "  bag = bag_of_words(pattern_sentence, all_words)\n",
        "  x_train.append(bag)\n",
        "\n",
        "  label = tags.index(tag)\n",
        "  y_train.append(label)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.n_samples = len(x_train)\n",
        "    self.x_data = x_train\n",
        "    self.y_data = y_train\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-qoKYYYB0wj",
        "outputId": "f5d51423-59b4-46fc-bba8-343d380dfb9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 patterns\n",
            "11 tags: ['accommodation', 'currency_exchange', 'dining', 'egypt_travel', 'emergency_services', 'goodbye', 'greeting', 'transportation', 'travel_booking', 'travel_tips', 'world_travel']\n",
            "154 unique stemmed words: ['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n",
            "['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating The Model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LNP7OQlEwsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        ""
      ],
      "metadata": {
        "id": "_97UufTbFAbj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using The Model"
      ],
      "metadata": {
        "id": "Ew3Ll6GhFzoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk_utils import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from model import NeuralNet\n",
        "\n",
        "with open('intents_training.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    # The problem was here. You need to iterate through intent['patterns'] not intents['patterns']\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "ignore_words = ['? ', '!', ',', '.']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n",
        "print(all_words)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "  bag = bag_of_words(pattern_sentence, all_words)\n",
        "  x_train.append(bag)\n",
        "\n",
        "  label = tags.index(tag)\n",
        "  y_train.append(label)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.n_samples = len(x_train)\n",
        "    self.x_data = x_train\n",
        "    self.y_data = y_train\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "input_size = len(x_train)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for (words, labels) in train_loader:\n",
        "    words = words.to(device)\n",
        "    labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "    outputs = model(words)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if (epoch+1) % 100 == 0:\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "sV2Gwf88F0yO",
        "outputId": "fb8ecb03-9715-491e-cfae-f1c460a7af7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 patterns\n",
            "11 tags: ['accommodation', 'currency_exchange', 'dining', 'egypt_travel', 'emergency_services', 'goodbye', 'greeting', 'transportation', 'travel_booking', 'travel_tips', 'world_travel']\n",
            "154 unique stemmed words: ['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n",
            "['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bb680c8dcc44>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import json\n",
        "from nltk_utils import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from model import NeuralNet\n",
        "\n",
        "with open('intents_training.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    # The problem was here. You need to iterate through intent['patterns'] not intents['patterns']\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        # Skip empty patterns\n",
        "        if not w:\n",
        "            continue\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "ignore_words = ['? ', '!', ',', '.']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n",
        "print(all_words)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    x_train.append(bag)\n",
        "\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(x_train)\n",
        "        self.x_data = x_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "input_size = len(x_train[0])  # Use the size of the bag-of-words vector\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(device)  # Labels are already long tensors\n",
        "\n",
        "        outputs = model(words)\n",
        "        # no_grad() is not needed here as we are in training mode\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_hNu-5uXHqgI",
        "outputId": "ac7c7c40-5353-4565-964c-6b66d0a2c03d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 patterns\n",
            "11 tags: ['accommodation', 'currency_exchange', 'dining', 'egypt_travel', 'emergency_services', 'goodbye', 'greeting', 'transportation', 'travel_booking', 'travel_tips', 'world_travel']\n",
            "154 unique stemmed words: ['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n",
            "['100', '?', 'a', 'about', 'abroad', 'accommod', 'advic', 'afford', 'afternoon', 'airbnb', 'alexandria', 'ambul', 'america', 'an', 'anyon', 'are', 'around', 'asia', 'aswan', 'attract', 'australia', 'avoid', 'bangkok', 'beach', 'best', 'book', 'budget-friendli', 'buse', 'bye', 'cafe', 'cairo', 'call', 'can', 'car', 'care', 'catch', 'cheap', 'cheapest', 'checklist', 'contact', 'convert', 'countri', 'cruis', 'currenc', 'deal', 'destin', 'dine', 'dish', 'do', 'dollar', 'dubai', 'easi', 'eat', 'egp', 'egypt', 'emerg', 'eur', 'euro', 'europ', 'even', 'exchang', 'family-friendli', 'find', 'fine', 'first-tim', 'flight', 'food', 'for', 'gbp', 'get', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'histor', 'hostel', 'hotel', 'how', 'i', 'in', 'inform', 'is', 'it', 'later', 'local', 'london', 'lost', 'luxor', 'luxuri', 'maldiv', 'me', 'medic', 'morn', 'much', 'need', 'new', 'next', 'nile', 'number', 'pack', 'pari', 'passport', 'place', 'polic', 'popular', 'public', 'pyramid', 'rate', 'rent', 'rental', 'resort', 'restaur', 's', 'safe', 'scam', 'see', 'servic', 'should', 'site', 'solo', 'south', 'stay', 'street', 'subway', 'take', 'tell', 'thank', 'the', 'there', 'time', 'tip', 'to', 'today', 'tokyo', 'top', 'top-rat', 'tour', 'tradit', 'transport', 'travel', 'usa', 'usd', 'use', 'visit', 'way', 'what', 'where', 'while', 'world', 'york', 'you', '’']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "must be real number, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e9c8517ad795>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Convert to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
          ]
        }
      ]
    }
  ]
}